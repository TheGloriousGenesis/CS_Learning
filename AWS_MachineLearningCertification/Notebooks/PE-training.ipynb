{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Exercise â€“ Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Spam email classification \n",
    "\n",
    "In this example, you will work on classifying emails as spam or not spam.\n",
    "\n",
    "You will use the [Spambase Data Set](https://archive.ics.uci.edu/ml/datasets/Spambase) from the University of California, Irvine (UCI) Machine Learning Repository.\n",
    "\n",
    "__Source information:__\n",
    "\n",
    "Creators:\n",
    "Mark Hopkins, Erik Reeber, George Forman, Jaap Suermondt, Hewlett-Packard Labs, 1501 Page Mill Rd., Palo Alto, CA 94304\n",
    "\n",
    "----------------------------------------------------------------------------\n",
    "\n",
    "The data has 57 attributes and 1 target. They are described below.\n",
    "- 48 continuous real [0,100] attributes of type word_freq_WORD \n",
    "= percentage of words in the e-mail that match WORD,\n",
    "i.e. 100 * (number of times the WORD appears in the e-mail) / \n",
    "total number of words in e-mail.  A \"word\" in this case is any \n",
    "string of alphanumeric characters bounded by non-alphanumeric \n",
    "characters or end-of-string.\n",
    "\n",
    "- 6 continuous real [0,100] attributes of type char_freq_CHAR\n",
    "= percentage of characters in the e-mail that match CHAR,\n",
    "i.e. 100 * (number of CHAR occurrences) / total characters in e-mail\n",
    "\n",
    "- 1 continuous real [1,...] attribute of type capital_run_length_average\n",
    "= average length of uninterrupted sequences of capital letters\n",
    "\n",
    "- 1 continuous integer [1,...] attribute of type capital_run_length_longest\n",
    "= length of longest uninterrupted sequence of capital letters\n",
    "\n",
    "- 1 continuous integer [1,...] attribute of type capital_run_length_total\n",
    "= sum of length of uninterrupted sequences of capital letters\n",
    "= total number of capital letters in the e-mail\n",
    "\n",
    "- 1 nominal {0,1} class attribute of type spam\n",
    "= denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update boto3 and SageMaker\n",
    "\n",
    "%pip install --upgrade boto3 sagemaker\n",
    "%reset -f\n",
    "import subprocess\n",
    "import os\n",
    "# Check whether the file is already in the desired path or if it needs to be downloaded\n",
    "# File downloaded from source : https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
    "\n",
    "base_path = '/home/ec2-user/SageMaker/data/'\n",
    "file_path = 'spambase.data'\n",
    "\n",
    "if not os.path.isfile(base_path + file_path):\n",
    "    subprocess.run(['mkdir', '-p', base_path])\n",
    "    subprocess.run(['aws', 's3', 'cp', \n",
    "                    's3://aws-tc-largeobjects/ILT-TF-200-MLDWTS/lab3/', \n",
    "                    base_path,'--recursive'])\n",
    "else:\n",
    "    print('File already downloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "data = []\n",
    "\n",
    "f = open('/home/ec2-user/SageMaker/data/spambase.data')\n",
    "reader = csv.reader(f)\n",
    "next(reader, None)\n",
    "for row in reader:\n",
    "    data.append(row)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the data into a DataFrame for easier visualization and analysis. Also, add the attribute names, which are from the \"spambase.names\" file in the dataset package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data=np.array(data).astype(np.float), columns=[\"word_freq_make\",\n",
    "            \"word_freq_address\",\n",
    "            \"word_freq_all\",\n",
    "            \"word_freq_3d\",\n",
    "            \"word_freq_our\",\n",
    "            \"word_freq_over\",\n",
    "            \"word_freq_remove\",\n",
    "            \"word_freq_internet\",\n",
    "            \"word_freq_order\",\n",
    "            \"word_freq_mail\",\n",
    "            \"word_freq_receive\",\n",
    "            \"word_freq_will\",\n",
    "            \"word_freq_people\",\n",
    "            \"word_freq_report\",\n",
    "            \"word_freq_addresses\",\n",
    "            \"word_freq_free\",\n",
    "            \"word_freq_business\",\n",
    "            \"word_freq_email\",\n",
    "            \"word_freq_you\",\n",
    "            \"word_freq_credit\",\n",
    "            \"word_freq_your\",\n",
    "            \"word_freq_font\",\n",
    "            \"word_freq_000\",\n",
    "            \"word_freq_money\",\n",
    "            \"word_freq_hp\",\n",
    "            \"word_freq_hpl\",\n",
    "            \"word_freq_george\",\n",
    "            \"word_freq_650\",\n",
    "            \"word_freq_lab\",\n",
    "            \"word_freq_labs\",\n",
    "            \"word_freq_telnet\",\n",
    "            \"word_freq_857\",\n",
    "            \"word_freq_data\",\n",
    "            \"word_freq_415\",\n",
    "            \"word_freq_85\",\n",
    "            \"word_freq_technology\",\n",
    "            \"word_freq_1999\",\n",
    "            \"word_freq_parts\",\n",
    "            \"word_freq_pm\",\n",
    "            \"word_freq_direct\",\n",
    "            \"word_freq_cs\",\n",
    "            \"word_freq_meeting\",\n",
    "            \"word_freq_original\",\n",
    "            \"word_freq_project\",\n",
    "            \"word_freq_re\",\n",
    "            \"word_freq_edu\",\n",
    "            \"word_freq_table\",\n",
    "            \"word_freq_conference\",\n",
    "            \"char_freq_;\",\n",
    "            \"char_freq_(\",\n",
    "            \"char_freq_[\",\n",
    "            \"char_freq_!\",\n",
    "            \"char_freq_$\",\n",
    "            \"char_freq_#\",\n",
    "            \"capital_run_length_average\",\n",
    "            \"capital_run_length_longest\",\n",
    "            \"capital_run_length_total\",\n",
    "            \"target\"]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the shape of the data. There are 58 columns (including the target) as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `df.describe()` function presents a statistical summary of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the model\n",
    "\n",
    "In this section, you will do the following:\n",
    "\n",
    "- Split the dataset into training, validation, and test subsets\n",
    "- Use the Amazon SageMaker linear learner algorithm to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Question 1 - Train-test-validation split\n",
    ">\n",
    "> An important part of training a machine learning model is splitting the data into training, validation, and test subsets. You will use the `train_test_split()` function from the `sklearn` library ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)).\n",
    "\n",
    ">Look at this example:\n",
    ">\n",
    "> `train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.2)` \n",
    ">\n",
    "> This function splits input feature `X` and target `y` pairs by a given ratio (test_size). This specific example splits the data into training (80%) and test (20%) subsets.\n",
    ">\n",
    "> Use this function __twice__ to split the data into __training (80%)__, __validation (10%)__, and __test (10%)__ subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get the feature values until the target column (not included)\n",
    "X = df.values[:, :-1].astype(np.float32)\n",
    "\n",
    "# Get the target column\n",
    "y = df.values[:, -1].astype(np.float32)\n",
    "\n",
    "# Get 80% of the data for training; the remaining 20% will be for validation and test\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Split the remaining 20% of data as 10% test and 10% validation\n",
    "test_features, val_features, test_labels, val_labels = train_test_split(test_features, test_labels, test_size=0.5)\n",
    "\n",
    "print(f\"Length of train_features is: {train_features.shape}\")\n",
    "print(f\"Length of train_labels is: {train_labels.shape}\")\n",
    "print(f\"Length of val_features is: {val_features.shape}\")\n",
    "print(f\"Length of val_labels is: {val_labels.shape}\")\n",
    "print(f\"Length of test_features is: {test_features.shape}\")\n",
    "print(f\"Length of test_labels is: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, call the Amazon SageMaker `LinearLearner()` algorithm. This example uses an `ml.m4.xlarge` instance for training. `predictor_type` is set to __'binary_classifier'__ because there are two classes: \"spam\" and \"not spam\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Call the LinearLearner estimator object\n",
    "binary_estimator = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                               instance_count=1,\n",
    "                                               instance_type='ml.m4.xlarge',\n",
    "                                               predictor_type='binary_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `record_set()` function of the binary_estimator to set the training, validation, and test parts of the estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records = binary_estimator.record_set(train_features, train_labels, channel='train')\n",
    "val_records = binary_estimator.record_set(val_features, val_labels, channel='validation')\n",
    "test_records = binary_estimator.record_set(test_features, test_labels, channel='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit()` function applies a distributed version of the Stochastic Gradient Descent (SGD) algorithm, and we are sending the data to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_estimator.fit([train_records, val_records, test_records])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluating the model\n",
    "\n",
    "In this section, you will look at how the model performs with the test dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon SageMaker analytics\n",
    "You can use `sagemaker.analytics` to get some performance metrics. This doesn't require deploying the model. Because this is a classification problem, you can check accuracy, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.analytics.TrainingJobAnalytics(binary_estimator._current_job_name, \n",
    "                                         metric_names = ['test:binary_classification_accuracy', \n",
    "                                                         'test:precision', \n",
    "                                                         'test:recall']\n",
    "                                        ).dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the model\n",
    "To deploy the model, run the following cell.\n",
    "\n",
    "**Note:** This takes some time to complete. While you are waiting, think about the answer to question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_predictor = binary_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Question 2 - Evaluation metric selection\n",
    "> You need to consider the business and its related restrictions when selecting the evaluation metric. In this case, spam detection requires being careful about false positives. A false positive means a good email is classified as a spam email. Because spam emails usually go to a spam folder, a user could lose important information from a false positive.\n",
    "\n",
    ">Some well-known evaluation metrics include the following:\n",
    "- Accuracy = ( TP + TN ) / ( TP + FP + FN + TN )\n",
    "- Precision = ( TP ) / ( TP + FP )\n",
    "- Recall = ( TP ) / ( TP + FN )\n",
    ">\n",
    ">In light of the provided information, which of the provided metrics do you think is the best to consider?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Precision is the choice of metric here as it accounts for the false positives. High precision means high TP and low FP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a hosted endpoint running, you can make real-time predictions from the model easily, simply by making an http POST request. First, you need to set up serializers and deserializers for passing the `test_features` NumPy arrays to the model behind the endpoint. You will also calculate the confusion matrix for the model to evaluate how it has done on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Question 3 - Calculate metrics\n",
    ">\n",
    "> In this question, you are asked to calculate some important evaluation metrics. The accuracy calculation is used as an example. Calculate the precision and recall after accuracy.\n",
    ">\n",
    ">**Hint:** You can check your calculated values with the `sagemaker.analytics` results you got in the beginning of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions for each batch with size of 25\n",
    "def predict_batches(predictor, features, labels):\n",
    "    prediction_batches = [predictor.predict(batch) for batch in np.array_split(features, 25)]\n",
    "    # Parse protobuf responses to extract predicted labels\n",
    "    extract_label = lambda x: x.label['predicted_label'].float32_tensor.values\n",
    "    preds = np.concatenate([np.array([extract_label(x) for x in batch]) for batch in prediction_batches])\n",
    "    preds = preds.reshape((-1,))\n",
    "\n",
    "    # Calculate accuracy, precision, and recall\n",
    "    accuracy = (preds == labels).sum() / len(labels)\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = (preds[preds == 1] == labels[preds == 1]).sum() / len(preds[preds == 1])\n",
    "    print(f'Precision: {precision}')\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = (preds[preds == 1] == labels[preds == 1]).sum() / len(labels[labels == 1])\n",
    "    print(f'Recall: {recall}')\n",
    "    \n",
    "    confusion_matrix = pd.crosstab(index=labels, columns=np.round(preds), rownames=['True'], colnames=['predictions']).astype(int)\n",
    "    plt.figure(figsize = (5,5))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='.2f', cmap=\"YlGnBu\").set_title('Confusion Matrix') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batches(binary_predictor, train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batches(binary_predictor, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Question 4 - Compare training-test results\n",
    "> Do you see a big difference between training and test performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The two performances look very similar. We didn't experience overfitting here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "In this exercise, you developed a binary classifier using the linear learning algorithm in Amazon SageMaker. You also reviewed important concepts such as training, validation, and test splits; confusion matrices; and classification metrics. Overall, the model resulted in over 90% in the accuracy, precision, and recall metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------\n",
    "# Problem 2: Diabetes progression prediction (regression)\n",
    "\n",
    "For regression, you will work with a health-related dataset to predict diabetes.\n",
    "\n",
    "You will use the [Diabetes Data](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html).\n",
    "\n",
    "The __Scikit-learn__ library hosts this data as well. You will use this library to load the dataset. \n",
    "\n",
    "Fields in this dataset:  \n",
    "\n",
    "Input:  \n",
    "1 - age  \n",
    "2 - sex  \n",
    "3 - bmi (Body mass index)  \n",
    "4 - bp (Blood pressure)  \n",
    "5 - s1 (Serum measurement 1)  \n",
    "6 - s2 (Serum measurement 2)  \n",
    "7 - s3 (Serum measurement 3)   \n",
    "8 - s4 (Serum measurement 4)  \n",
    "9 - s5 (Serum measurement 5)  \n",
    "10 - s6 (Serum measurement 6)\n",
    "\n",
    "Output:  \n",
    "11 - y (A quantitative measure of disease progression one year after baseline)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data.astype(np.float32), diabetes.target.astype(np.float32)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data=np.column_stack((X, y)), \n",
    "                  columns=['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'y']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The features are normalized and scaled beforehand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "for col in ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']:\n",
    "    plt.hist(df[col].values)\n",
    "    plt.title(col + ' distribution')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Counts')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "plt.hist(y)\n",
    "plt.title('Target values')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('Counts')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the model\n",
    "\n",
    "In this section, you will do the following:\n",
    "\n",
    "- Split the dataset into training, validation, and test subsets\n",
    "- Use the Amazon SageMaker linear learner algorithm to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Question 1 - Train-test-validation split\n",
    ">\n",
    "> An important part of training a machine learning model is splitting the data into training, validation, and test subsets. You will use the `train_test_split()` function from the `sklearn` library [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "\n",
    ">Look at this example:\n",
    ">\n",
    "> `train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.2)` \n",
    ">\n",
    "> This function splits input feature `X` and target `y` pairs by a given ratio (test_size). This specific example splits the data into training (80%) and test (20%) subsets.\n",
    ">\n",
    "> Use this function __twice__ to split the data into __training (80%)__, __validation (10%)__, and __test (10%)__ subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get 80% of the data for training; the remaining 20% will be for validation and test\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Split the remaining 20% of data as 10% test and 10% validation\n",
    "test_features, val_features, test_labels, val_labels = train_test_split(test_features, test_labels, test_size=0.5)\n",
    "\n",
    "print(f\"Length of train_features is: {train_features.shape}\")\n",
    "print(f\"Length of train_labels is: {train_labels.shape}\")\n",
    "print(f\"Length of val_features is: {val_features.shape}\")\n",
    "print(f\"Length of val_labels is: {val_labels.shape}\")\n",
    "print(f\"Length of test_features is: {test_features.shape}\")\n",
    "print(f\"Length of test_labels is: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, call the Amazon SageMaker `LinearLearner()` algorithm. This example uses an `ml.m4.xlarge` instance for training. `predictor_type` is set to __'regressor'__ because this is a regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Call the LinearLearner estimator object\n",
    "regression_model = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                               instance_count=1,\n",
    "                                               instance_type='ml.m4.xlarge',\n",
    "                                               predictor_type='regressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `record_set()` function of the regression_model to set the training, validation, and test parts of the estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records = regression_model.record_set(train_features, train_labels, channel='train')\n",
    "val_records = regression_model.record_set(val_features, val_labels, channel='validation')\n",
    "test_records = regression_model.record_set(test_features, test_labels, channel='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit()` function applies regression on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model.fit([train_records, val_records, test_records])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluating the model\n",
    "\n",
    "In this section, you will look at how your model performs with the test dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon SageMaker analytics\n",
    "You can use `sagemaker.analytics` to get some performance metrics. This doesn't require deploying the model. Because this is a regression problem, you can check the **mean squared error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.analytics.TrainingJobAnalytics(regression_model._current_job_name, \n",
    "                                         metric_names = ['test:mse']\n",
    "                                        ).dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the model\n",
    "To deploy the model, run the following cell.\n",
    "\n",
    "**Note:** This takes some time to complete. While you are waiting, think about the answer to question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_predictor = regression_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Question 2 - Calculate mean squared error\n",
    ">\n",
    ">In this question, you will calculate the mean squared error. It is given by this equation:\n",
    ">\n",
    ">mean squared error = $\\displaystyle\\frac{1}{n}\\sum_{t=1}^{n}(pred_t-target_t)^2$\n",
    ">\n",
    ">Calculate the mean squared error using the __preds__ and __labels__ variables and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predictions for each batch with size of 25\n",
    "def predict_regression_batches(predictor, features, labels):\n",
    "    prediction_batches = [predictor.predict(batch) for batch in np.array_split(features, 25)]\n",
    "    \n",
    "    # Parse protobuf responses to extract predicted labels\n",
    "    extract_score = lambda x: x.label['score'].float32_tensor.values\n",
    "    preds = np.concatenate([np.array([extract_score(x) for x in batch]) for batch in prediction_batches])\n",
    "    preds = preds.reshape((-1,))\n",
    "    \n",
    "    # Calculate mean squared error\n",
    "    mse = sum((preds - labels)**2) / len(preds)\n",
    "    print(mse)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get the results on the training and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = predict_regression_batches(regression_predictor, train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = predict_regression_batches(regression_predictor, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results\n",
    "\n",
    "Let's look at some plots. Even though you calculated the mean squared error, it is still a good idea to examine the results visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot\n",
    "\n",
    "With the scatter plot, you look at the fitted (predicted) values vs. true values. From the following plot, you can see that the predicted and true values followed a similar pattern overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "plt.scatter(train_predictions, train_labels, c='b', s=40, alpha=0.25, label='Training')\n",
    "plt.scatter(test_predictions, test_labels, c='g', s=40, label='Test')\n",
    "plt.title(\"Scatter plot of Fitted vs True values\")\n",
    "plt.ylabel(\"True values\")\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual plot\n",
    "\n",
    "Every data point has a residual. It is the difference between the real value and the predicted value. Residuals are calculated like this:  \n",
    "$e = y - \\hat y$  \n",
    "\n",
    "A positive residual means a higher prediction than true value, and a negative residual means a lower prediction than true value.    \n",
    "\n",
    "Take a look at the residual plot from running the following block. There is a random pattern in residuals, which is good. Other patterns you might see are the U-shape or inverted U-shape patterns. These two patterns point out some non-linearity in the data, which needs to be fixed. In this case case, we don't have that problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "plt.scatter(train_predictions, train_predictions-train_labels, c='b', s=40, alpha=0.25, label='Training')\n",
    "plt.scatter(test_predictions, test_predictions-test_labels, c='g', s=40, label='Test')\n",
    "plt.hlines(y=0, xmin=0, xmax=300)\n",
    "plt.title(\"Residual plots\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Question 3: Residual plot assessment\n",
    ">\n",
    ">In general, looking at a residual plot allows you to realize a pattern. Patterns include:\n",
    ">- Random: This means the data is linear.\n",
    ">- U or inverse U shape: This points out some non-linearity in the data.\n",
    ">\n",
    ">What can you conclude from the residual plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The plot shows random residuals, meaning the data is linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "In this exercise, you developed a regression model using the linear learning algorithm in Amazon SageMaker. You also covered important concepts, such as training, validation, and test splits; the mean squared error regression metric; and analysis of residual error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
